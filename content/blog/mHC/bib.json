[
  {"id":"heDeepResidualLearning2016","abstract":"Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.","accessed":{"date-parts":[["2026",1,19]]},"author":[{"family":"He","given":"Kaiming"},{"family":"Zhang","given":"Xiangyu"},{"family":"Ren","given":"Shaoqing"},{"family":"Sun","given":"Jian"}],"citation-key":"heDeepResidualLearning2016","container-title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2016.90","event-title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4673-8851-1","issued":{"date-parts":[["2016",6]]},"language":"en","page":"770–778","publisher":"IEEE","publisher-place":"Las Vegas, NV, USA","source":"DOI.org (Crossref)","title":"Deep Residual Learning for Image Recognition","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7780459/"},
  {"id":"huangDenselyConnectedConvolutional2017a","accessed":{"date-parts":[["2026",1,19]]},"author":[{"family":"Huang","given":"Gao"},{"family":"Liu","given":"Zhuang"},{"family":"Van Der Maaten","given":"Laurens"},{"family":"Weinberger","given":"Kilian Q."}],"citation-key":"huangDenselyConnectedConvolutional2017a","container-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2017.243","event-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-5386-0457-1","issued":{"date-parts":[["2017",7]]},"language":"en","page":"2261–2269","publisher":"IEEE","publisher-place":"Honolulu, HI","source":"DOI.org (Crossref)","title":"Densely Connected Convolutional Networks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8099726/"},
  {"id":"jiangSpectralNormDoubly2024","abstract":"A simple proof using Birkhoff theorem is given for the result that the spectral norm of a doubly stochastic matrix is 1. We also show that the result generalizes the results of İpek, Bozkurt, and Jiang and Zhou on circulant matrices and r r -circulant matrices. Spectral norm of level- k k circulant matrix and applications are given.","accessed":{"date-parts":[["2026",1,15]]},"author":[{"family":"Jiang","given":"Zhao-Lin"},{"family":"Tam","given":"Tin-Yau"}],"citation-key":"jiangSpectralNormDoubly2024","container-title":"Special Matrices","DOI":"10.1515/spma-2023-0106","ISSN":"2300-7451","issue":"1","issued":{"date-parts":[["2024",1,1]]},"language":"en","license":"De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.","publisher":"De Gruyter Open Access","source":"www.degruyterbrill.com","title":"On the spectral norm of a doubly stochastic matrix and level-k circulant matrix","type":"article-journal","URL":"https://www.degruyterbrill.com/document/doi/10.1515/spma-2023-0106/html","volume":"12"},
  {"id":"liVisualizingLossLandscape2018","abstract":"Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","accessed":{"date-parts":[["2026",1,14]]},"author":[{"family":"Li","given":"Hao"},{"family":"Xu","given":"Zheng"},{"family":"Taylor","given":"Gavin"},{"family":"Studer","given":"Christoph"},{"family":"Goldstein","given":"Tom"}],"citation-key":"liVisualizingLossLandscape2018","DOI":"10.48550/arXiv.1712.09913","issued":{"date-parts":[["2018",11,7]]},"number":"arXiv:1712.09913","publisher":"arXiv","source":"arXiv.org","title":"Visualizing the Loss Landscape of Neural Nets","type":"article","URL":"http://arxiv.org/abs/1712.09913"},
  {"id":"nylenNumericalRangeDoubly1991","abstract":"We discuss the numerical range of a doubly stochastic matrix and relate it to the structure of the matrix. Also, we characterize those subsets of Q which are the numerical range of a 3 x 3 doubly stochastic matrix.","accessed":{"date-parts":[["2026",1,15]]},"author":[{"family":"Nylen","given":"Peter"},{"family":"Tam","given":"Tin-Yau"}],"citation-key":"nylenNumericalRangeDoubly1991","container-title":"Linear Algebra and Its Applications","container-title-short":"Linear Algebra Appl.","DOI":"10.1016/0024-3795(91)90216-J","ISSN":"00243795","issued":{"date-parts":[["1991",7]]},"language":"en","license":"https://www.elsevier.com/tdm/userlicense/1.0/","page":"161–176","source":"DOI.org (Crossref)","title":"Numerical range of a doubly stochastic matrix","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/002437959190216J","volume":"153"},
  {"id":"sinkhornConcerningNonnegativeMatrices1967","accessed":{"date-parts":[["2026",1,14]]},"author":[{"family":"Sinkhorn","given":"Richard"},{"family":"Knopp","given":"Paul"}],"citation-key":"sinkhornConcerningNonnegativeMatrices1967","container-title":"Pacific Journal of Mathematics","container-title-short":"Pacific J. Math.","DOI":"10.2140/pjm.1967.21.343","ISSN":"0030-8730, 0030-8730","issue":"2","issued":{"date-parts":[["1967",5,1]]},"language":"en","page":"343–348","source":"DOI.org (Crossref)","title":"Concerning nonnegative matrices and doubly stochastic matrices","type":"article-journal","URL":"http://msp.org/pjm/1967/21-2/p14.xhtml","volume":"21"},
  {"id":"vaswaniAttentionAllYou2017","accessed":{"date-parts":[["2026",1,19]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N"},{"family":"Kaiser","given":"Ł","dropping-particle":"ukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2017","container-title":"Advances in Neural Information Processing Systems","DOI":"10.48550/arXiv.1706.03762","event-title":"NeurIPS'17","issued":{"date-parts":[["2017"]]},"language":"en","page":"5998–6008","publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Attention is All you Need","type":"paper-conference","URL":"https://papers.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html","volume":"30"},
  {"id":"veitResidualNetworksBehave2016","accessed":{"date-parts":[["2026",1,14]]},"author":[{"family":"Veit","given":"Andreas"},{"family":"Wilber","given":"Michael J"},{"family":"Belongie","given":"Serge"}],"citation-key":"veitResidualNetworksBehave2016","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2016"]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Residual Networks Behave Like Ensembles of Relatively Shallow Networks","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2016/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html","volume":"29"},
  {"id":"xieMHCManifoldConstrainedHyperConnections2026","abstract":"Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.","accessed":{"date-parts":[["2026",1,14]]},"author":[{"family":"Xie","given":"Zhenda"},{"family":"Wei","given":"Yixuan"},{"family":"Cao","given":"Huanqi"},{"family":"Zhao","given":"Chenggang"},{"family":"Deng","given":"Chengqi"},{"family":"Li","given":"Jiashi"},{"family":"Dai","given":"Damai"},{"family":"Gao","given":"Huazuo"},{"family":"Chang","given":"Jiang"},{"family":"Yu","given":"Kuai"},{"family":"Zhao","given":"Liang"},{"family":"Zhou","given":"Shangyan"},{"family":"Xu","given":"Zhean"},{"family":"Zhang","given":"Zhengyan"},{"family":"Zeng","given":"Wangding"},{"family":"Hu","given":"Shengding"},{"family":"Wang","given":"Yuqing"},{"family":"Yuan","given":"Jingyang"},{"family":"Wang","given":"Lean"},{"family":"Liang","given":"Wenfeng"}],"citation-key":"xieMHCManifoldConstrainedHyperConnections2026","DOI":"10.48550/arXiv.2512.24880","issued":{"date-parts":[["2026",1,5]]},"language":"en","number":"arXiv:2512.24880","publisher":"arXiv","source":"arXiv.org","title":"mHC: Manifold-Constrained Hyper-Connections","title-short":"mHC","type":"article","URL":"http://arxiv.org/abs/2512.24880"},
  {"id":"xuDevelopmentSkipConnection2025","abstract":"Deep learning has made significant progress in computer vision, specifically in image classification, object detection, and semantic segmentation. The skip connection has played an essential role in the architecture of deep neural networks,enabling easier optimization through residual learning during the training stage and improving accuracy during testing. Many neural networks have inherited the idea of residual learning with skip connections for various tasks, and it has been the standard choice for designing neural networks. This survey provides a comprehensive summary and outlook on the development of skip connections in deep neural networks. The short history of skip connections is outlined, and the development of residual learning in deep neural networks is surveyed. The effectiveness of skip connections in the training and testing stages is summarized, and future directions for using skip connections in residual learning are discussed. Finally, we summarize seminal papers, source code, models, and datasets that utilize skip connections in computer vision, including image classification, object detection, semantic segmentation, and image reconstruction. We hope this survey could inspire peer researchers in the community to develop further skip connections in various forms and tasks and the theory of residual learning in deep neural networks. The project page can be found at https://github.com/apple1986/Residual_Learning_For_Images","accessed":{"date-parts":[["2026",1,14]]},"author":[{"family":"Xu","given":"Guoping"},{"family":"Wang","given":"Xiaxia"},{"family":"Wu","given":"Xinglong"},{"family":"Leng","given":"Xuesong"},{"family":"Xu","given":"Yongchao"}],"citation-key":"xuDevelopmentSkipConnection2025","container-title":"Engineering Applications of Artificial Intelligence","container-title-short":"Engineering Applications of Artificial Intelligence","DOI":"10.1016/j.engappai.2024.109890","ISSN":"09521976","issued":{"date-parts":[["2025",2]]},"language":"en","page":"109890","source":"arXiv.org","title":"Development of Skip Connection in Deep Neural Networks for Computer Vision and Medical Image Analysis: A Survey","title-short":"Development of Skip Connection in Deep Neural Networks for Computer Vision and Medical Image Analysis","type":"article-journal","URL":"http://arxiv.org/abs/2405.01725","volume":"142"},
  {"id":"zhuHyperConnections2025","abstract":"We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.","accessed":{"date-parts":[["2026",1,14]]},"author":[{"family":"Zhu","given":"Defa"},{"family":"Huang","given":"Hongzhi"},{"family":"Huang","given":"Zihao"},{"family":"Zeng","given":"Yutao"},{"family":"Mao","given":"Yunyao"},{"family":"Wu","given":"Banggu"},{"family":"Min","given":"Qiyang"},{"family":"Zhou","given":"Xun"}],"citation-key":"zhuHyperConnections2025","DOI":"10.48550/arXiv.2409.19606","issued":{"date-parts":[["2025",3,18]]},"number":"arXiv:2409.19606","publisher":"arXiv","source":"arXiv.org","title":"Hyper-Connections","type":"article","URL":"http://arxiv.org/abs/2409.19606"}
]
